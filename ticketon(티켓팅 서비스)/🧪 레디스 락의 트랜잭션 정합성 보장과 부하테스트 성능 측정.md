###  목적  
  
결제 완료 이후 **티켓 수량 차감 및 결제 정보 저장 과정**에서 **데이터 정합성을 보장**하면서도 **실시간 서비스를 운영할 수 있는 성능을 확보**하기 위한 락 처리 방식을 실험하고 비교하였습니다.  
  
- **외부 PG사 호출 제외**  
- **동시 요청 수: 100명**  
- **각 요청의 트랜잭션 지연 시간: 20초**  
- **테스트 도구: k6**  
  
---

### 테스트 스크립트

```javascript
import http from 'k6/http';  
import { check,sleep } from 'k6';  
import { uuidv4 } from 'https://jslib.k6.io/k6-utils/1.4.0/index.js';  // UUID 라이브러리  
  
  
export let options = {  
    vus : 100,  
    duration : '15s',  
};  
  
export default function(){  
   const url = 'http://host.docker.internal:8081/v1/api/payments/confirm'  
   const uniqueId = uuidv4();  
  
   const payload = JSON.stringify({  
     paymentKey: "test-key",  
     orderId: uniqueId,  
     amount: 10000,  
     ticketTypeId: 1,  
     memberId: Math.floor(Math.random() * 100) + 1,  
   });  
  
   const headers = {'Content-Type': 'application/json'};  
  
   const res = http.post(url,payload,{headers});  
  
   check(res, {  
    'is status 200': (r) => r.status === 200 || r.status === 201,  
    '응답 메시지 확인': (r) => r.body.includes("잔여 수량") || r.body.includes("성공"),  
   });  
  
   sleep(1);  
}
```

 **실행 명령어**
 ` docker run --rm \ -v "$(pwd)/stress-test/k6-scripts:/scripts" \ grafana/k6 run - influxdb=http://host.docker.internal:8086/k6 /scripts/lockTest.js` 
 
 - docker 로 influxdb 와 grafana 를 띄웠을 때는, docker 내부에서 동작을 실시해야 한다.
 
___
### 기존 아키텍처  
  
1. Kafka 결제 완료 토픽에서 메시지 수신  
2. **메시지 하나당 한 번의 DB 작업 수행 **
  
###  비관적 락 (Pessimistic Lock)  
  
- DB의 `SELECT ... FOR UPDATE`를 사용하여 티켓 수량 업데이트를 직렬 처리  
- 하나의 Consumer에서 순차적으로 트랜잭션 처리

![사진](./image/pessimisticlock.png)

####  결과  
- 데이터 정합성: **100% 보장**  
- 실패 요청률: **0%**  
- 평균 응답 시간: **약 32초**  
- 단점: 사용자가 응답을 받기까지 대기 시간이 매우 김  
  
---  
  
###  레디스 기반 락 (Redis Lock)  
  
- Redisson을 사용한 분산 락 적용  
- 트랜잭션 진입 전 Redis 락을 획득하여 동시성 제어

![사진](./image/redisLock.png)
####  결과  
- 데이터 정합성: **100% 보장**  
- 평균 응답 시간:  
  - 락 대기시간 5초 설정 시: **비관적 락보다 느림**  
    - 락 대기시간 1초 설정 시: **약 25초**로 개선  
- 단점: 락 획득 실패 시 재시도 필요, 락 해제 및 경쟁 로직 복잡  
  
---  
  
###  배치 처리 + Redis 락 (Batch Processing + Redis Lock)  
  
기존 구조에서는 Kafka Consumer가 하나의 메시지를 받고 매번 락을 걸어 처리했습니다.    
이를 개선하여 **여러 메시지를 모아서 한 번에 처리**하고, **락과 DB 접근을 최소화**하여 성능을 향상시켰습니다.  
  
####  개선 아키텍처  
  
1. Kafka 결제 완료 토픽에서 메시지를 **버퍼링**  
2. **일정량 수신되면 한 번만 락을 걸어 배치로 DB 처리**  
3. 병렬 Consumer (`setConcurrency(3)`) 및 Kafka 파티션 병렬성을 함께 활용

![사진](./image/fixedRedisLock.png)
####  결과  
- 데이터 정합성: **100% 보장**  
- 평균 응답 시간: **약 4초**  
- 성능 향상: **기존 대비 약 80% 개선**  
- 장점: 락 횟수 및 DB I/O 최소화, Kafka 병렬성 극대화  
  
**배치 처리 기반 Redis 락 방식**은 데이터 정합성과 실시간 응답성이라는 두 가지 요구를 동시에 만족시킬 수 있는 방식으로, 실제 서비스 환경에서 충분한 성능을 보장합니다.

비즈니스 로직 상 병목 현상과 성능 저하는 **Lock 이 걸리는 부분에서 집중적**으로 발생함을 알 수 있습니다.
이를 개선하기 위해 컨슈머 갯수와 파티션 갯수를 늘리는 방법과 여러 가지 다른 방법들을 생각해보면 좋을 것 같습니다.

___

### 락 획득 실패 시 



___
## ⚠️ 트러블 슈팅(부하테스트 진행 시 주의할 점)

### 문제상황
여러 번의 테스트 이후에 자꾸 `ExceededTicketQuantityException.java` 에러가 발생
DB 안에 데이터 없었음
#### 원인
Kafka Consumer 내의 처리되지 않은 메시지들이 남아있는 상황에서 초기화가 되지 않아 이전의 메시지들을 처리 하느라 데이터가 계속 쌓여 있었음

**해결방안**
1. docker container 를 전부 셧다운 시키고 다시 재실행
	-> `docker compose down -v`

- **이 방법은 운영 환경에서는 절대 안됨**

2. 기존 컨슈머 그룹의 이름을 새로운 이름으로 변경
- application.yml 의 group-id 이름을 바꾸고 재배포

